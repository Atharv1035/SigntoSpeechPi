{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b370b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 10:45:15.293201: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-14 10:45:15.302778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744607715.314489  148060 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744607715.317891  148060 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744607715.326493  148060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744607715.326510  148060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744607715.326511  148060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744607715.326512  148060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-14 10:45:15.329816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1744607717.174153  148060 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 929 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_name = '255_labels0.8953'\n",
    "loaded_model = load_model(f\"../models/Model_{model_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62413e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (Input, Bidirectional, LSTM, BatchNormalization, Dropout, \n",
    "                          MultiHeadAttention, Flatten, Dense, LeakyReLU, Conv1D, MaxPooling1D, GlobalAveragePooling1D)\n",
    "from keras.regularizers import l2\n",
    "\n",
    "input_shape = loaded_model.input_shape[1:]  # Get shape from loaded model\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "#Convolution Layer\n",
    "conv = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "conv = MaxPooling1D(pool_size=2)(conv)\n",
    "# CPU-friendly LSTM (unroll=True, explicit activations)\n",
    "lstm_out1 = Bidirectional(LSTM(256, return_sequences=True, activation='tanh',\n",
    "                               recurrent_activation='sigmoid', unroll=True))(conv)\n",
    "lstm_out1 = BatchNormalization()(lstm_out1)\n",
    "lstm_out1 = Dropout(0.2)(lstm_out1)\n",
    "\n",
    "lstm_out2 = Bidirectional(LSTM(256, return_sequences=True, activation='tanh',\n",
    "                               recurrent_activation='sigmoid', unroll=True))(lstm_out1)\n",
    "lstm_out2 = BatchNormalization()(lstm_out2)\n",
    "\n",
    "lstm_out3 = Bidirectional(LSTM(256, return_sequences=True, activation='tanh',\n",
    "                               recurrent_activation='sigmoid', unroll=True))(lstm_out2)\n",
    "lstm_out3 = BatchNormalization()(lstm_out3)\n",
    "\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=64)(lstm_out3, lstm_out3)\n",
    "\n",
    "pooled=GlobalAveragePooling1D()(attention_out)\n",
    "\n",
    "dense1 = Dense(128, kernel_regularizer=l2(1e-4))(pooled)\n",
    "dense1 = LeakyReLU()(dense1)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "dense1 = Dropout(0.2)(dense1)\n",
    "\n",
    "dense2 = Dense(64, kernel_regularizer=l2(1e-4))(dense1)\n",
    "dense2 = LeakyReLU()(dense2)\n",
    "dense2 = BatchNormalization()(dense2)\n",
    "dense2 = Dropout(0.2)(dense2)\n",
    "\n",
    "output_layer = Dense(loaded_model.output_shape[1], activation='softmax')(dense2)\n",
    "\n",
    "model_cpu = Model(inputs=input_layer, outputs=output_layer)\n",
    "model_cpu.set_weights(loaded_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9493debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxp3vwqs6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxp3vwqs6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpxp3vwqs6'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 543, 3), dtype=tf.float32, name='keras_tensor_41')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 255), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127604964367824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964365360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964375392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604965052960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604965059120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964736016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964721232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964736192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964725808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964722992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964857232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964440048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964437584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964658864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964659392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964656048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964663792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964660800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964664672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964665728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964659216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604964663088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889533456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889531872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889528000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889529584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889533632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889530464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889571168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889569760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889567296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889570464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889577504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889574336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889576800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889578032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889579616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889580496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889583312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889682512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889690080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889686912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889691488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889689376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889690608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889692016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889696944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604889697648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819513888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819511248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819509664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819513184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819521456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127604819518288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1744607845.439187  148060 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1744607845.439204  148060 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-04-14 10:47:25.439361: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxp3vwqs6\n",
      "2025-04-14 10:47:25.461408: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-04-14 10:47:25.461422: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpxp3vwqs6\n",
      "2025-04-14 10:47:25.859116: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-04-14 10:47:26.593339: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpxp3vwqs6\n",
      "2025-04-14 10:47:27.243394: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 1804037 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_cpu)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(f\"../models/{model_name}.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
